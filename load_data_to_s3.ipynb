{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"92c5639a-b3f1-4acb-b157-e13375cda59c","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import udf,col\n","from pyspark.sql.types import StructType, StructField, StringType\n","import os\n","import psycopg2\n","\n","spark = SparkSession \\\n","        .builder \\\n","        .appName(\"Create delta tables\") \\\n","        .getOrCreate()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"12447f41-a19d-40fe-b98e-fe1ff691ec9e","showTitle":false,"title":""}},"outputs":[],"source":["# Use the database demodb in the catalog hive_metastore\n","spark.sql(\"USE DATABASE demodb;\")\n","\n","# Queries to get data from the postgres database\n","queries = [{\n","        \"table\":\"airports_data\",\n","        \"query\":\"SELECT a.airport_code, a.airport_name, a.city, a.coordinates FROM public.airports_data a LIMIT 200\",\n","        \"columns\":[\"airport_code\", \"airport_name\", \"city\", \"coordinates\"] }\n","    ,{   \"table\":\"aircrafts_data\", \n","        \"query\":\" SELECT b.aircraft_code, b.model, b.range FROM public.aircrafts_data b LIMIT 200\",\n","        \"columns\":[ \"aircraft_code\", \"model\",\"range\"] } ]\n","\n","def query_database(query):\n","    \"\"\"Get data from postgres database on the database hosting server.\n","    Args:\n","        query(Str): The query for getting data from postgres database.\n","    return: \n","        my_cursor(cursor):Returns data.\n","    \"\"\"\n","    my_connection = psycopg2.connect(user=\"******\", password=\"*****\", host=\"******\",port=\"5432\",database=\"******\")\n","    my_cursor = my_connection.cursor()\n","    my_cursor.execute(query)\n","    return my_cursor.fetchall()\n","\n","def data_postgres_database_to_delta_table():\n","    \"\"\"Get data from the postgres database, and save as delta table.\n","    Args:\n","        table_name(Str): The table name which contains data.\n","        columns(list): List of columns name of the table.\n","    \"\"\"\n","    for t in queries:\n","        df = spark.createDataFrame(query_database(t[\"query\"])).toDF(*t[\"columns\"])\n","        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(t[\"table\"])\n","    \n","def dbfs_to_delta_table(path_dbfs):\n","    \"\"\"Get data from DBFS.\n","    Args:\n","        path_dbfs(Str): The path where csv file are on Databrick File System\n","    \"\"\"\n","    list_of_csv_file = os.listdir(path_dbfs)\n","    location = path_dbfs.replace(\"/dbfs\",\"\")\n","    for file in list_of_csv_file:\n","        drop_table = \"DROP TABLE IF EXISTS \" + file\n","        spark.sql(drop_table)\n","        df = spark.read.format(\"csv\") \\\n","            .option(\"inferSchema\", \"true\") \\\n","            .option(\"header\", \"true\") \\\n","            .option(\"sep\",\",\") \\\n","            .load(location + \"/\" + file)\n","        df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(file[:file.index(\".\")])\n","    \n","data_postgres_database_to_delta_table()\n","dbfs_to_delta_table(\"/dbfs/data_of_demodb\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"68012011-c335-4176-9a20-553d029ef191","showTitle":false,"title":""}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"31aa8358-b185-4d7a-9580-7c46c826f5c9","showTitle":false,"title":""}},"outputs":[],"source":["# data_of_demodb is a directory of the bucket AWS s3 specified when creating a metastore.\n","\n","location = \"/dbfs/data_of_demodb\"\n","\n","list_of_csv_file = os.listdir(location)\n","\n","location_for_table = location.replace(\"/dbfs\",\"\")\n","\n","for file in list_of_csv_file:\n","    drop_table = \"DROP TABLE IF EXISTS \" + file\n","    spark.sql(drop_table)\n","    df = spark.read.format(\"csv\") \\\n","        .option(\"inferSchema\", \"true\") \\\n","        .option(\"header\", \"true\") \\\n","        .option(\"sep\",\",\") \\\n","        .load(location_for_table + \"/\" + file)\n","    df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(file[:file.index(\".\")])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0bebaee5-f2e0-4a69-89ba-30fc7236220d","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n","\u001b[0;32m<command-45300716577359>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n","\u001b[1;32m     10\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     11\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_for_table\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\n","\u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n","\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   1043\u001b[0m     def json(\n","\n","\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n","\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n","\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n","\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;31mAnalysisException\u001b[0m: Table `aircrafts` already exists."]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m<command-45300716577359>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_for_table\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     def json(\n\n\u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: Table `aircrafts` already exists.","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Table `aircrafts` already exists.","errorTraceType":"ansi","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["\n","\n","spark.sql(\"USE catalog hive_metastore\")\n","\n","location = \"/dbfs/data_of_demodb\"\n","list_ = os.listdir(location)\n","location_for_table = location.replace(\"/dbfs\",\"\")\n","for file in list_of_csv_file:\n","    df = spark.read.format(\"csv\") \\\n","        .option(\"inferSchema\", \"true\") \\\n","        .option(\"header\", \"true\") \\\n","        .option(\"sep\",\",\") \\\n","        .load(location_for_table + \"/\" + file)\n","    df.write.format(\"parquet\").saveAsTable(file[:file.index(\".\")])"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":-1,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"load_data_to_s3","notebookOrigID":2288822831207585,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
